{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Coder Hussam Qassim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build 3D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.\n",
    "First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "'''\n",
    "'''\n",
    "Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did before. The following code \n",
    "applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically \n",
    "takes care of centering the data)\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "'''\n",
    "After fitting the PCA transformer to the dataset, you can access the principal components using the components_\n",
    "variable (note that it contains the PCs as horizontal vectors, so, for example, the first principal component \n",
    "is equal to pca.components_.T[:, 0] )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c40c10c3dd0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malong\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maxis\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mprincipal\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Another very useful piece of information is the explained variance ratio of each principal component, available\n",
    "via the explained_variance_ratio_ variable. It indicates the proportion of the dataset’s variance that lies \n",
    "along the axis of each principal component\n",
    "'''\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "'''\n",
    "This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6% lies along the second\n",
    "axis. This leaves less than 1.2% for the third axis, so it is reasonable to assume that it probably carries \n",
    "little information\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91e63d3a054a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m '''\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to\n",
    "choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).\n",
    "Unless, of course, you are reducing dimensionality for data visualization — in that case you will generally \n",
    "want to reduce the dimensionality down to 2 or 3. The following code computes PCA without reducing \n",
    "dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s \n",
    "variance\n",
    "'''\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "'''\n",
    "You could then set n_components=d and run PCA again. However, there is a much better option: instead of \n",
    "specifying the number of principal components you want to preserve, you can set n_components to be a float \n",
    "between 0.0 and 1.0 , indicating the ratio of variance you wish to preserve\n",
    "'''\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "'''\n",
    "Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot\n",
    "cumsum). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can\n",
    "think of this as the intrinsic dimensionality of the dataset. In this case, you can see that reducing the \n",
    "dimensionality down to about 100 dimensions wouldn’t lose too much explained variance\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Obviously after dimensionality reduction, the training set takes up much less space. For example, try applying \n",
    "PCA to the MNIST dataset while preserving 95% of its variance. You should find that each instance will have \n",
    "just over 150 features, instead of the original 784 features. So while most of the variance is preserved, \n",
    "the dataset is now less than 20% of its original size! This is a reasonable compression ratio, and you can see \n",
    "how this can speed up a classification algorithm (such as an SVM classifier) tremendously. It is also possible \n",
    "to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA \n",
    "projection. Of course this won’t give you back the original data, since the projection lost a bit of \n",
    "information (within the 5% variance that was dropped), but it will likely be quite close to the original data. \n",
    "The mean squared distance between the original data and the reconstructed data (compressed and then \n",
    "decompressed) is called the reconstruction error. For example, the following code compresses the MNIST dataset\n",
    "down to 154 dimensions, then uses the inverse_transform() method to decompress it back to 784 dimensions. \n",
    "'''\n",
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One problem with the preceding implementation of PCA is that it requires the whole training set to fit in\n",
    "memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have\n",
    "been developed: you can split the training set into mini-batches and feed an IPCA algorithm one\tmini-\n",
    "batch at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\n",
    "instances arrive). The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s \n",
    "array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class 5 to reduce the dimensionality \n",
    "of the MNIST dataset down to 154 dimensions (just like before). Note that you must call the partial_fit()\n",
    "method with each mini-batch rather than the fit() method with the whole training set\n",
    "'''\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)\n",
    "\n",
    "'''\n",
    "Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a large array stored in a \n",
    "binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when\n",
    "it needs it. Since the IncrementalPCA class uses only a small part of the array at any given time, the memory\n",
    "usage remains under control. This makes it possible to call the usual fit() method, as you can see in the \n",
    "following code\n",
    "'''\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm\n",
    "that quickly finds an approximation of the first d principal components. Its computational complexity is\n",
    "O(m × d2 ) + O(d3 ), instead of O(m × n2) + O(n3), so it is dramatically faster than the previous algorithms\n",
    "when d is much smaller than n\n",
    "'''\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We discussed the kernel trick, a mathematical technique that implicitly maps instances into a very \n",
    "high-dimensional space (called the feature space), enabling nonlinear classification and regression with \n",
    "Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature space \n",
    "corresponds to a complex nonlinear decision boundary in the original space. It turns out that the same trick\n",
    "can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction.\tThis\tis\tcalled\tKernel\tPCA\t(kPCA). 6 \tIt\tis\toften\tgood\tat\n",
    "preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a \n",
    "twisted manifold. For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA with \n",
    "an RB kernel\n",
    "'''\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As\tkPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best\n",
    "kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a \n",
    "supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and \n",
    "hyperparameters that lead to the best performance on that task. For example, the following code creates a \n",
    "two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic \n",
    "Regression for classification. Then it uses GridSearchCV to find the best kernel and gamma value for kPCA in\n",
    "order to get the best classification accuracy at the end of the pipeline\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "            (\"kpca\", KernelPCA(n_components=2)),\n",
    "            (\"log_reg\",\tLogisticRegression())\n",
    "        ])\n",
    "param_grid = [{\n",
    "            \"kpca__gamma\":\tnp.linspace(0.03,\t0.05,\t10),\n",
    "            \"kpca__kernel\":\t[\"rbf\",\t\"sigmoid\"]\n",
    "        }]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# The best kernel and hyperparameters are then available through the best_params_ variable\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "'''\n",
    "Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters that yield\n",
    "the lowest reconstruction error. However, reconstruction is not as easy as with linear PCA. Here’s why. Thanks\n",
    "to the kernel trick, this is mathematically equivalent to mapping the training set to an infinite-dimensional\n",
    "feature space using the feature map φ, then projecting the transformed training set down to 2D using linear PCA.\n",
    "Notice that if we could invert the linear PCA step for a given instance in the reduced space, the reconstructed\n",
    "point would lie in feature space, not in the original space (e.g., like the one represented by an x in the \n",
    "diagram). Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, and \n",
    "therefore we cannot compute the true reconstruction error. Fortunately, it is possible to find a point in the\n",
    "original space that would map close to the reconstructed point. This is called the reconstruction pre-image.\n",
    "Once you have this pre-image, you can measure its squared distance to the original instance. You can then\n",
    "select the kernel and hyperparameters that minimize this reconstruction pre-image error. You may be wondering\n",
    "how to perform this reconstruction. One solution is to train a supervised regression model, with the projected\n",
    "instances as the training set and the original instances as the targets. Scikit-Learn will do this \n",
    "automatically if you set fit_inverse_transform=True , as shown in the following code\n",
    "'''\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "'''\n",
    "NOTE: By default, fit_inverse_transform=False and KernelPCA has no inverse_transform() method. This method \n",
    "only gets created when you set fit_inverse_transform=True.\n",
    "'''\n",
    "# You can then compute the reconstruction pre-image error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(X, X_preimage)\n",
    "\n",
    "'''\n",
    "Now you can use grid search with cross-validation to find the kernel and hyperparameters that minimize this\n",
    "pre-image reconstruction error\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Locally Linear Embedding (LLE) is another very powerful nonlinear dimensionality reduction (NLDR) technique.\n",
    "It is a Manifold Learning technique that does not rely on projections like the previous algorithms. In a \n",
    "nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors \n",
    "(c.n.), and then looking for a low-dimensional representation of the training set where these local \n",
    "relationships are best preserved (more details shortly). This makes it particularly good at unrolling twisted\n",
    "manifolds, especially when there is not too much noise. For example, the following code uses Scikit-Learn’s \n",
    "LocallyLinearEmbedding class to unroll the Swiss roll. As you can see, the Swiss roll is completely unrolled\n",
    "and the distances between instances are locally well preserved. However, distances are not preserved on a \n",
    "larger scale: the left part of the unrolled Swiss roll is squeezed, while the right part is stretched.\n",
    "Nevertheless, LLE did a pretty good job at modeling the manifold.\n",
    "'''\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn.\n",
    "Here are some of the most popular:\n",
    "\n",
    "- Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve the distance between the \n",
    "instances. \n",
    "\n",
    "- Isomap creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality \n",
    "while trying to preserve the geodesic distances 9 between the instances. \n",
    "\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar \n",
    "instances close and dissimilar instances apart. It is mostly used for visualization, in particular to \n",
    "visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
    "\n",
    "- Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it learns\n",
    "the most discriminative axes between the classes, and these axes can then be used to define a hyperplane\n",
    "onto which to project the data. The benefit is that the projection will keep classes as far apart as possible,\n",
    "so LDA is a good technique to reduce dimensionality before running another classification algorithm such as \n",
    "an SVM classifier.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
